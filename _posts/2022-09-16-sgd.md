---
layout: post
title:  First-Order Optimization (SGD & Variants)
date: 2022-09-16 19:00:00 +0800
permalink: /post/sgd/
---

# First-Order Optimization (SGD & Variants)

In deep learning, it is common to minimize a **loss function**. This poses an **optimization** problem. 

A very common **optimizing algorithm** is the **stochastic gradient descent (SGD)**. There are a lot of materials covering it, so we will not dive deep into it. We will instead address the basic idea of it and its variants:

* [**SGD**](#sgd)
* [**SGD + Momentum**](#sgd--momentum)
* [**SGD + Nesterov Momentum**](#sgd--nesterov-momentum)
* [**Adagrad**](#adagrad)
* [**RMSProp**](#rmsprop)
* [**Adam**](#adam) 

They are all **first-order optimization**, meaning only first derivatives are used.

### SGD

Whenever we have a multivariate **loss funtion** $$\mathcal{L}_\theta (\hat{x}, x)$$ with input $$x$$, output $$\hat{x}$$ and weight $$\theta$$, we can minimize it with respect to wight $$\theta$$ by taking the negative **gradient** of it to find the steepest "slope" to the minimum (not neccessary a global one) and walks towards it,

$$-\nabla_\theta \mathcal{L}(\theta) = -\nabla_\theta \mathcal{L}_\theta (\hat{x}, x)$$

For learning with batches (batch size = $$N$$), it is the average sum of the **gradient** for each pair of $$x_i$$ and $$\hat{x}i$$, 

$$
\begin{align*}
-\nabla_\theta \mathcal{L}(\theta) &= -\nabla_\theta \mathcal{L}_\theta (\hat{x}, x) \\
&= -\nabla_\theta \frac{1}{N} \sum_{i=0}^N \mathcal{L}_\theta (\hat{x}_i, x_i) \\
&= -\frac{1}{N} \sum_{i=0}^N \nabla_\theta \mathcal{L}_\theta (\hat{x}_i, x_i)
\end{align*}$$

The algorithm for **SGD** can be written simply as,

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta \mathcal{L}(\theta_t)$$

where $$\alpha$$ is the **learning rate**. The setting of it will be covered later in [here](#learning_rate).

### Problems with SGD

There are THREE main problems with **SGD**,

* Very slow progress along shallow dimension, jitter along steep direction

![sgd_jitter](\assets\img\sgd_jitter.png "SGD Jitter")

* Stuck in **local minima** or **saddle point** ($$\nabla \mathcal{L}=0$$)

![sgd_local_min](\assets\img\sgd_local_min.png "SGD Local Minima")

![sgd_saddle](\assets\img\sgd_saddle.png "SGD Saddle Point")

* Gradients from batches (average sum) can be noisy

$$-\nabla_\theta \frac{1}{N} \sum_{i=0}^N \mathcal{L}_\theta (\hat{x}_i, x_i) = -\frac{1}{N} \sum_{i=0}^N \nabla_\theta \mathcal{L}_\theta (\hat{x}_i, x_i)$$


### SGD + Momentum

Imagine we are driving a car, when we want to change direction, we cannot change at a 90-degree angle. There is **momentum**, which is that we continue moving in the general direction as the previous iterations.

![sgd_momentum](\assets\img\sgd_momentum.png "SGD + Momentum")

The algorithm can be written as

$$
\begin{align*}
v_{t+1} &= \rho v_t + \nabla_\theta \mathcal{L}(\theta_t) \\
\theta_{t+1} &= \theta_t - \alpha v_{t+1}
\end{align*}
$$

where $$\rho$$ gives “friction” and $$\alpha$$ is the [**learning rate**](#learning_rate). "Friction" is the literal meaning to our car analogy. The higher the "friction" $$\rho$$, the lower the momentum velocity $$v$$. 

Typically, $$\rho = 0.9$$ or $$0.99$$.

An *alternative* way for the algorithm, 

$$
\begin{align*}
v_{t+1} &= \rho v_t - \alpha \nabla_\theta \mathcal{L}(\theta_t) \\
\theta_{t+1} &= \theta_t + \alpha v_{t+1}
\end{align*}
$$

where the [**learning rate**](#learning_rate) $$\alpha$$ only affects the **gradient** $$-\nabla_\theta \mathcal{L}(\theta_t)$$.
 
### SGD + Nesterov Momentum

Instead of calculating the **gradient** for the current $$\theta_t$$, we can "look ahead" and calculate the **gradient** at the tip of the momentum velocity $$v$$.

![sgd_nesterov](\assets\img\sgd_nesterov_momentum.png "SGD + Nesterov Momentum")

Using the *alternatve* way described above, the algorithm can be written as

$$
\begin{align*}
v_{t+1} &= \rho v_t - \alpha \nabla_\theta \mathcal{L}(\theta_t + \rho v_t) \\
\theta_{t+1} &= \theta_t + v_{t+1}
\end{align*}
$$

where $$\rho$$ gives “friction” and $$\alpha$$ is the [**learning rate**](#learning_rate). 

The term $$\theta_t + \rho v_t$$ is a bit annoying, we let

$$\tilde{\theta}_t = \theta_t + \rho v_t$$

Substitutng into \tilde{\theta}_{t+1},

$$
\begin{align*}
\tilde{\theta}_{t+1} &= \theta_{t+1} + \rho v_{t+1} \\
&= ( \theta_t + v_{t+1} ) + \rho v_{t+1} \\
&= ( \tilde{\theta}_t - \rho v_t + v_{t+1} ) + \rho v_{t+1} \\
&= \tilde{\theta}_t - \rho v_t + (1 + \rho)v_{t+1} 
\end{align*}
$$

So the algorithm is, 

$$
\begin{align*}
v_{t+1} &= \rho v_t - \alpha \nabla_\theta \mathcal{L}(\tilde{\theta}_t) \\
\tilde{\theta}_{t+1} &= \tilde{\theta}_t - \rho v_t + (1 + \rho)v_{t+1} \\
\text{or } &= \tilde{\theta}_t + v_{t+1} + \rho ( v_{t+1} - v_t )
\end{align*}
$$

### Adagrad

Let's get back to the original [**SGD**](#sgd), we can consider a historical **sum of squares** term $$SS(\nabla)$$ for the **gradient** $$-\nabla_\theta \mathcal{L}(\theta_t)$$ from time $$T=0$$ to $$T=t$$,

$$SS(\nabla) = \sum_{T=0}^t ( \nabla_\theta \mathcal{L}(\theta_T) )^2$$

The algorithm becomes

$$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{SS(\nabla)} + 0.1^7} \nabla_\theta \mathcal{L}(\theta_t)$$

where $$\alpha$$ is the **learning rate**. 

Why do we divide by $$\sqrt{SS(\nabla)}$$?

* Progress along steep directions is damped while progress along shallow dimensions is accelerated..

![sgd_jitter](\assets\img\sgd_jitter.png "SGD Jitter")

* Over time, $$\displaystyle \frac{1}{\sqrt{SS(\nabla)}} \to 0$$. So the weight $$\theta$$ won't get updated anymore afterwards.

Why add $$0.1^7$$ to the *denominator*?

* When time $$T=0$$, the denominator will not be $$0$$ when $$SS(\nabla) = 0$$.

### RMSProp

Continuing with [**Adagrad**](#adagrad), we can add a **decaying rate** $$\beta$$ to the historical **sum of squares** term $$SS(\nabla)$$.

$$SS(\nabla) = \beta \sum_{T=0}^{t-1} ( \nabla_\theta \mathcal{L}(\theta_T) )^2 
+ (1- \beta)( \nabla_\theta \mathcal{L}(\theta_t) )^2$$

where the **decaying rate** $$\beta$$ is applied to all the histroical **sum of squares** from time $$T=0$$ to $$T=t-1$$.

The algorithm is the same,

$$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{SS(\nabla)} + 0.1^7} \nabla_\theta \mathcal{L}(\theta_t)$$

### Adam 

Refer to [this paper](https://arxiv.org/pdf/1412.6980.pdf){:target="_blank"} for details.

We now call the historical **sum of squares** term $$SS(\nabla)$$ as **second moment** $$m_2$$, with the **decaying rate** $$\beta_2$$.

$$m_2 = SS(\nabla) = \beta_2 \sum_{T=0}^{t-1} ( \nabla_\theta \mathcal{L}(\theta_T) )^2 
+ (1- \beta_2)( \nabla_\theta \mathcal{L}(\theta_t) )^2$$

What about the **first moment** $$m_1$$?

It is the historical **sum** (no squares) of the **gradient** $$\nabla_\theta \mathcal{L}(\theta_T)$$ with the adding of **decaying rate** $$\beta_1$$,

$$m_1 = \beta_1 \sum_{T=0}^{t-1} \nabla_\theta \mathcal{L}(\theta_T) 
+ (1- \beta_1)\nabla_\theta \mathcal{L}(\theta_t)$$

We then divide both **first moment** $$m_1$$ and **second moment** $$m_2$$ with an **unbias term** $$(1-\beta_1^{\,\,t})$$ and $$(1-\beta_2^{\,\,t})$$ respectively, the two **unbiased moments** $$\hat{m}_1$$ and $$\hat{m}_2$$ are

$$
\begin{align*}
\hat{m}_1 &= \frac{1}{(1-\beta_1^{\,\,t})} m_1 \\
\hat{m}_2 &= \frac{1}{(1-\beta_2^{\,\,t})} m_2 
\end{align*}
$$

We do this because **first moment** $$m_1$$ and **second moment** $$m_2$$ estimates start at zero.

The algorithm becomes

$$\theta_{t+1} = \theta_t - \alpha \frac{\hat{m}_1 }{ \sqrt{\hat{m}_2 } + 0.1^7}$$

where $$\alpha$$ is the **learning rate**, $$\beta_1$$ and $$\beta_2$$ are the **decaynig rate** for the **first moment** $$m_1$$ and **second moment** $$m_2$$ respectively. 

Typically, we starts with $$\beta_1 = 0.9$$, $$\beta_2 = 0.999$$ and **learning rate** $$\alpha = 0.1^3$$.

### Learning Rate

How shall we set the **learning rate** $$\alpha$$?

![learning rate schedule](\assets\img\learning_rate_schedule.png "Learning rate Schedule")

In fact, they all can be good. We can learn more at the beginning and adjust fine details towards the end.

We can have **decaying learning rate**. We can do it with a few methods. For the following, $$t$$ is the current epoch and $$T$$ is the total number of epochs

* We can reduce **learning rate**$$\alpha$$ at a fixed point.  
(i.e. multiply $$\alpha$$ by $$0.1$$ at epoch $$30, 60, 90, \dots$$)

![fixed reduce](\assets\img\learning_rate_fixed_reduce.png "Reducing at fixed point")

* We can use **cosine**

$$\alpha_t = \frac12 \alpha_0 \left(1+\cos\left(\frac{t\pi}{T}\right)\right)$$

![cosine_1](\assets\img\learning_rate_cosine_1.png "Reducing with cosine")

![cosine_2](\assets\img\learning_rate_cosine_2.png "Reducing with cosine Loss")

* We can use **linear**

$$\alpha_t = \alpha_0 \left(1 - \frac{t}{T} \right)$$

![linear](\assets\img\learning_rate_linear.png "Reducing linearly")

* We can use **inverse square root**

$$\alpha_t = \frac{\alpha_0}{\sqrt{t}}$$

![inverse sqrt](\assets\img\learning_rate_inverse_sqrt.png "Reducing with inverse square root")